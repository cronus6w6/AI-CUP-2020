{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train",
      "provenance": [],
      "mount_file_id": "1Bt9EThnilNr6U63C7NcugtRLsbUK3Hbp",
      "authorship_tag": "ABX9TyPqoZIKZnyP9SsWEh+i2z/H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cronus6w6/AI-CUP-2020/blob/master/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV38Huk1bJ3j"
      },
      "source": [
        "#準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1emH_WfFbTrd"
      },
      "source": [
        "##安裝及引入"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hv6OM6HsjnQ9"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvBfSAjxla8s"
      },
      "source": [
        "from transformers import *\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "import shutil\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from tqdm import tqdm\r\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-tiQ7L7bfNd"
      },
      "source": [
        "## 參數調整"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnXwetpsblaM"
      },
      "source": [
        "train_data_path = \"trainset.csv\"  # train dataset path\r\n",
        "LABELS = [\"THEORETICAL\", \"ENGINEERING\", \"EMPIRICAL\", \"OTHERS\"]\r\n",
        "Epochs = 5\r\n",
        "lr = 1e-5\r\n",
        "thrld = [0.35, 0.3, 0.25, 0.35]\r\n",
        "positive_weights = [1., 1., 1.75, 7.5]\r\n",
        "dropout = 0.2\r\n",
        "hidden_unit = 64\r\n",
        "# seed = random.randint(0, 100000)\r\n",
        "seed = 29231"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6iYwXe6cK8S"
      },
      "source": [
        "##初始化設定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxRsQHqxcKMn"
      },
      "source": [
        "torch.manual_seed(seed)\r\n",
        "torch.cuda.manual_seed(seed)\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuUKp1WVcgVL"
      },
      "source": [
        "#資料處理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MACubcR9c6bB"
      },
      "source": [
        "宣告資料集類別"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCdjlfRWclAx"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\r\n",
        "class _DataSet(torch.utils.data.Dataset):\r\n",
        "  def __init__(self, inp_data: pd.DataFrame):\r\n",
        "    title_indices = []\r\n",
        "    title_segments = []\r\n",
        "    abstract_indices = []\r\n",
        "    abstract_segments = []\r\n",
        "    self.return_labels = \"Classifications\" in inp_data.columns\r\n",
        "    if self.return_labels:\r\n",
        "      labels = []\r\n",
        "    for _, row in tqdm(inp_data.iterrows(), total=len(inp_data)):\r\n",
        "      title_index = tokenizer.encode(row.Title, max_length=512, padding=\"max_length\")\r\n",
        "      abstract_index = tokenizer.encode(row.Abstract, max_length=512, padding=\"max_length\", truncation=True)\r\n",
        "      title_indices.append(title_index)\r\n",
        "      abstract_indices.append(abstract_index)\r\n",
        "      if self.return_labels:\r\n",
        "        labels.append(list(map(lambda l: 1 if l in row.Classifications.split(\" \") else 0, LABELS)))\r\n",
        "        self.labels = torch.tensor(labels, dtype=torch.float32, device=device)\r\n",
        "    self.title_indices = torch.tensor(title_indices, dtype=torch.long, device=device)\r\n",
        "    self.title_segments = torch.zeros(self.title_indices.size(), dtype=torch.long, device=device)\r\n",
        "    self.abstract_indices = torch.tensor(abstract_indices, dtype=torch.long, device=device)\r\n",
        "    self.abstract_segments = torch.zeros(self.abstract_indices.size(), dtype=torch.long, device=device)\r\n",
        "  def __getitem__(self, index):\r\n",
        "    if self.return_labels:\r\n",
        "      return ({\r\n",
        "        \"title_indices\": self.title_indices[index],\r\n",
        "        \"title_segments\": self.title_segments[index],\r\n",
        "        \"abstract_indices\": self.abstract_indices[index],\r\n",
        "        \"abstract_segments\": self.abstract_segments[index]\r\n",
        "      }, self.labels[index])\r\n",
        "    return {\r\n",
        "      \"title_indices\": self.title_indices[index],\r\n",
        "      \"title_segments\": self.title_segments[index],\r\n",
        "      \"abstract_indices\": self.abstract_indices[index],\r\n",
        "      \"abstract_segments\": self.abstract_segments[index]\r\n",
        "    }\r\n",
        "  def __len__(self):\r\n",
        "    return len(self.title_indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gm-AbUIQdBic"
      },
      "source": [
        "切詞"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zVr4V7ccfj-"
      },
      "source": [
        "trainset = pd.read_csv(train_data_path)\r\n",
        "trainset.Abstract = trainset.Abstract.str.replace(\"\\$\\$\\$\", \" \")\r\n",
        "train_data = _DataSet(trainset.iloc[:6300])\r\n",
        "val_data = _DataSet(trainset.iloc[6300:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajefaMxbdsR3"
      },
      "source": [
        "製作batch loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LYrWhBXdqGX"
      },
      "source": [
        "train_data_loader = torch.utils.data.DataLoader(train_data, 5)\r\n",
        "val_data_loader = torch.utils.data.DataLoader(val_data, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "008BWfGcd0aK"
      },
      "source": [
        "# 模型製作"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8OWm_POd4MQ"
      },
      "source": [
        "class MultiClassificationModel(nn.Module):\r\n",
        "  def __init__(self, encoder):\r\n",
        "    super(MultiClassificationModel, self).__init__()\r\n",
        "    self.encoder = encoder\r\n",
        "    self.classifier = nn.Sequential(\r\n",
        "        nn.Linear(768 * 2, hidden_unit),\r\n",
        "        nn.GELU(),\r\n",
        "        nn.Dropout(dropout),\r\n",
        "        nn.Linear(hidden_unit, 4)\r\n",
        "    )\r\n",
        "\r\n",
        "  def forward(self, title_indices, title_segments, abstract_indices, abstract_segments):\r\n",
        "    title_embs = self.encoder(title_indices, token_type_ids=title_segments)[0][:, 0, :]\r\n",
        "    abstract_embs = self.encoder(abstract_indices, token_type_ids=abstract_segments)[0][:, 0, :]\r\n",
        "    \r\n",
        "    embs = torch.cat([title_embs, abstract_embs], 1)\r\n",
        "    result = self.classifier(embs)\r\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tSwMqeseI8A"
      },
      "source": [
        "scibert = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')\r\n",
        "model = MultiClassificationModel(scibert)\r\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tshAUf1ueUds"
      },
      "source": [
        "#訓練"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_dWNL35hBM8"
      },
      "source": [
        "F1 Score 計算函數"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kfuqga_vhArv"
      },
      "source": [
        "def micro_f1_score(pred, label):\r\n",
        "    \r\n",
        "    TP = torch.mul(pred, label)[0]\r\n",
        "    FP = (torch.mul(pred, (label-1)) != 0)[0]\r\n",
        "    FN = (torch.mul(pred-1, label) != 0)[0]\r\n",
        "    \r\n",
        "    precision = TP.sum() / (TP.sum() + FP.sum())\r\n",
        "    recall = TP.sum() / (TP.sum() + FN.sum())\r\n",
        "    \r\n",
        "    f1 = 2 * precision * recall / (precision + recall)\r\n",
        "    \r\n",
        "    return f1, TP, FP, FN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKIfIC8MeY6V"
      },
      "source": [
        "pos_weight = torch.FloatTensor(positive_weights).to(device)\r\n",
        "total_step = len(train_data_loader) * Epochs\r\n",
        "warmup_step = total_step // 2\r\n",
        "\r\n",
        "optimizer =  AdamW(model.parameters(), lr=lr, correct_bias=False)\r\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps= warmup_step, num_training_steps=total_step)\r\n",
        "best_f1 = -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vPzaGZdeoOc"
      },
      "source": [
        "for epoch in range(Epochs):\r\n",
        "  print(f\"Epoch {epoch}:\")\r\n",
        "  \r\n",
        "  # Train\r\n",
        "  model = model.train()\r\n",
        "  criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\r\n",
        "  thrld = np.array(thrld)\r\n",
        "  total_loss = 0.0\r\n",
        "  train_metric = { \"TP\":np.zeros(4), \"FP\":np.zeros(4), \"FN\":np.zeros(4), \"F1\":[] }\r\n",
        "  thrld_ten = torch.from_numpy(thrld).float().to(device)\r\n",
        "  train_metric[\"TP\"] = torch.from_numpy(train_metric[\"TP\"]).float().to(device)\r\n",
        "  train_metric[\"FP\"] = torch.from_numpy(train_metric[\"FP\"]).float().to(device)\r\n",
        "  train_metric[\"FN\"] = torch.from_numpy(train_metric[\"FN\"]).float().to(device)\r\n",
        "\r\n",
        "  optimizer.zero_grad()\r\n",
        "  for features, labels in tqdm(train_data_loader, total=len(train_data_loader), desc=\"Training...\"):\r\n",
        "    optimizer.step()\r\n",
        "    scheduler.step()\r\n",
        "    optimizer.zero_grad()\r\n",
        "\r\n",
        "    result = model(**features)\r\n",
        "    l = criterion(result, labels)\r\n",
        "    total_loss += l.item()\r\n",
        "    l.backward()\r\n",
        "    \r\n",
        "    result = torch.sigmoid(result)\r\n",
        "    pred = (result > thrld_ten.expand(labels.size())).float()\r\n",
        "    f1, tp, fp, fn = micro_f1_score(pred, labels)\r\n",
        "\r\n",
        "    train_metric[\"F1\"].append(f1)\r\n",
        "    train_metric[\"TP\"] += tp.float()\r\n",
        "    train_metric[\"FP\"] += fp.float()\r\n",
        "    train_metric[\"FN\"] += fn.float()\r\n",
        "\r\n",
        "  optimizer.step()\r\n",
        "  scheduler.step()\r\n",
        "\r\n",
        "  train_precision_all = train_metric[\"TP\"].sum().item() / (train_metric[\"TP\"].sum().item() + train_metric[\"FP\"].sum().item())\r\n",
        "  train_recall_all = train_metric[\"TP\"].sum().item() / (train_metric[\"TP\"].sum().item() + train_metric[\"FN\"].sum().item())\r\n",
        "  train_micro_f1 = (2 * train_precision_all * train_recall_all) / (train_precision_all + train_recall_all)\r\n",
        "\r\n",
        "  avg_loss = total_loss / len(train_data_loader)\r\n",
        "\r\n",
        "  print(\"Train Loss:{}\\tmicro_f1:{}\".format(avg_loss, train_micro_f1))\r\n",
        "  print(\"micro_f1s:\", end=\"\")\r\n",
        "  for i in range(4):\r\n",
        "    precision = train_metric[\"TP\"][i].item() / (train_metric[\"TP\"][i].item()+train_metric[\"FP\"][i].item()+1e-10)\r\n",
        "    recall = train_metric[\"TP\"][i].item() / (train_metric[\"TP\"][i].item()+train_metric[\"FN\"][i].item()+1e-10)\r\n",
        "    print(\"{}\".format(2*precision*recall / (precision+recall+1e-10)), end=\"\\t\")\r\n",
        "\r\n",
        "  print(\"\")\r\n",
        "\r\n",
        "  #Evaluation\r\n",
        "  model = model.eval()\r\n",
        "  criterion = nn.BCEWithLogitsLoss()\r\n",
        "  \r\n",
        "  dev_loss = 0.0\r\n",
        "  dev_metric = { \"TP\":np.zeros(4), \"FP\":np.zeros(4), \"FN\":np.zeros(4), \"F1\":[] }\r\n",
        "  dev_metric[\"TP\"] = torch.from_numpy(dev_metric[\"TP\"]).float().to(device)\r\n",
        "  dev_metric[\"FP\"] = torch.from_numpy(dev_metric[\"FP\"]).float().to(device)\r\n",
        "  dev_metric[\"FN\"] = torch.from_numpy(dev_metric[\"FN\"]).float().to(device)\r\n",
        "\r\n",
        "  with torch.no_grad():\r\n",
        "    for features, labels in tqdm(val_data_loader, total=len(val_data_loader), desc=\"Training...\"):\r\n",
        "      optimizer.step()\r\n",
        "      scheduler.step()\r\n",
        "      optimizer.zero_grad()\r\n",
        "\r\n",
        "      result = model(**features)\r\n",
        "      l = criterion(result, labels)\r\n",
        "      dev_loss += l\r\n",
        "      \r\n",
        "      result = torch.sigmoid(result)\r\n",
        "\r\n",
        "      pred = (result > thrld_ten.expand(labels.size())).float()\r\n",
        "      f1, tp, fp, fn = micro_f1_score(pred, labels)\r\n",
        "\r\n",
        "      dev_metric[\"F1\"].append(f1)\r\n",
        "      dev_metric[\"TP\"] += tp.float()\r\n",
        "      dev_metric[\"FP\"] += fp.float()\r\n",
        "      dev_metric[\"FN\"] += fn.float()\r\n",
        "\r\n",
        "\r\n",
        "  dev_precision_all = dev_metric[\"TP\"].sum().item() / (dev_metric[\"TP\"].sum().item() + dev_metric[\"FP\"].sum().item())\r\n",
        "  dev_recall_all = dev_metric[\"TP\"].sum().item() / (dev_metric[\"TP\"].sum().item() + dev_metric[\"FN\"].sum().item())\r\n",
        "  dev_micro_f1 = (2 * dev_precision_all * dev_recall_all) / (dev_precision_all + dev_recall_all)\r\n",
        "\r\n",
        "  avg_loss = dev_loss / len(train_data_loader)\r\n",
        "\r\n",
        "  print(\"Train Loss:{}\\tmicro_f1:{}\".format(avg_loss, dev_micro_f1))\r\n",
        "  print(\"micro_f1s:\", end=\"\")\r\n",
        "  for i in range(4):\r\n",
        "    precision = dev_metric[\"TP\"][i].item() / (dev_metric[\"TP\"][i].item()+dev_metric[\"FP\"][i].item()+1e-10)\r\n",
        "    recall = dev_metric[\"TP\"][i].item() / (dev_metric[\"TP\"][i].item()+dev_metric[\"FN\"][i].item()+1e-10)\r\n",
        "    print(\"{}\".format(2*precision*recall / (precision+recall+1e-10)), end=\"\\t\")\r\n",
        "\r\n",
        "  print(\"\")\r\n",
        "  if dev_micro_f1 > best_f1:\r\n",
        "    best_f1 = dev_micro_f1\r\n",
        "print(\"Best F1: {}\".format(best_f1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xUstjQXe1EF"
      },
      "source": [
        "torch.save(model.state_dict(), \"model_state_{}\".format(seed))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}